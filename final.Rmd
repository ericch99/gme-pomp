---
title: "Information Epidemics: Modeling Search Trends during the GameStop Short Squeeze Using Stochastic Compartmental Models"
subtitle: STATS 531 Final Project (W21)
author: "Eric Chen, Bella Chen"
output:
  html_document:
    toc: yes
    toc_depth: 2
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	warning = FALSE,
	message = FALSE,
	include = TRUE,
	comment='',
	fig.align = "center"
)
```

```{r load_packages, include=FALSE}
library(pomp)
library(tidyverse)

set.seed(1350254336)

library(foreach)
library(doParallel)
registerDoParallel()
library(doRNG)
registerDoRNG(12345678)

read_csv('gme.csv') -> gme
read_csv('price.csv') -> price
```


<div style='color:red'>
TODO:

- Section 4 (Parameter Search)
  * find optimal parameters
  * compute optimal likelihood
  * what else?

- Section 5 (Diagnostics)
  * scatterplot matrix
  * profile likelihoods
  * what else?

- Section 6 (Stock Price)
  * all of 6.2 - mathematical formulation of model
  * all of 6.3 - global search, again
  * all of 6.4 - likelihood ratio test

- Section 7 (Conclusions)

- Section 8 (Limitations)

</div>



# 1. Introduction

Social scientists have long drawn comparisons between the spread of information and the spread of disease. Methods from epidemiology have been used to model such dynamics by treating an idea, meme, or news event as the infectious agent; this analogy has only grown in popularity with the explosive growth of social media  [[1](https://www.sciencedirect.com/science/article/pii/S0307904X11002824), [2](https://engineering.stanford.edu/magazine/article/how-fake-news-spreads-real-virus)].

In this project, we apply epidemiological methods -- specifically, compartmental models -- to model information flow during the 2021 GameStop short squeeze, an event in which thousands of Reddit users coordinated to buy shares of the gaming company GameStop, having anticipated a short by major hedge funds. The resulting jump in stock price caused such hedge funds to incur significant losses, resulting in a flurry of attention on both social media sites and major news networks [[3](https://en.wikipedia.org/wiki/GameStop_short_squeeze)].

We use search frequency of the term "gme", GameStop's ticker symbol, as a proxy for the spread of news. The data was downloaded from Google Trends [[4](https://trends.google.com/trends/)], consisting of normalized daily measurements over an 88-day period. We also include GameStop's stock price (\$GME) for the same time period, downloaded from Yahoo Finance [[5](https://finance.yahoo.com/)].

The main objectives of this project are as follows: 

1. Model the information spread of a current event using an interpretable model.

2. Draw parallels between the spread of information and the spread of disease.

Additionally, it may be interesting to investigate if the stock price can be viewed as an external driver for the GME hype. Thus, we seek to:

3. Determine the relationship between GameStop's stock price and search frequency.

------------

# 2. Exploratory Data Analysis

```{r data cleaning, include=FALSE}
gme$count[as.numeric(gme$count) %>% is.na()] <- 0
gme$count <- as.numeric(gme$count)
price$High[price$High %>% is.na()] <- 0
```


```{r dataplot, echo=FALSE}
gme %>% ggplot(aes(x=date,y=count)) + geom_line() + geom_point() + ggtitle('Search Frequency of "gme", 1/6/2021 – 4/3/2021') +theme(plot.title = element_text(hjust = 0.5))
```
  
First, we note that the search data are normalized to have a maximum value of $100$. Understandably, the search frequency of the term "gme" peaks in late January, during the initial short squeeze. Notably, the data strongly resemble the measles dataset studied in class, lending support to the idea that such trends can be modeled using methods from epidemiology.

The initial peak is followed by multiple, smaller peaks in the following months. This latter half of the data seems to exhibit strong weekly seasonality, as shown in the plot of the smoothed periodogram below. However, this may be because many stock exchanges are closed on weekends, and not something directly attributable to the short squeeze itself  [[6](https://www.investopedia.com/terms/w/weekendeffect.asp)]. 

(Later on, we will directly investigate how the stock price is related to the search frequency in the <span style='color:red'>[SECTION NAME]</span> section.)

```{r spectrum, echo=FALSE}
smoothed <- spectrum(gme$count[45:88], spans=c(5,5,5), main="Periodogram (Smoothed) - Past 43 Days")
abline(v=1/7, lty=2, col='red')
paste0('period corresponding to maximum spectral density (days): ', 1 / smoothed$freq[which.max(smoothed$spec)])
```

Combined with the evidence from the periodogram, the erratic, quasi-regular trend in the latter half of the data suggests that we may want to choose a model that accounts for both regular and irregular resurgences (or even delays) in popularity.

------------

# 3. Model Design

## 3.1. Model Structure

Given our analyses above, we find it appropriate to model our data with a compartmental model, commonly used in epidemiological studies -- specifically, we use a **SIRS model with delay**.

We define *susceptible* in this context to mean individuals who have not yet heard about the short squeeze, *infected* to mean individuals who have heard about the short squeeze and are actively interested in it, and *recovered* to refer to individuals who have lost interest in the short squeeze and have no further inclination to search about it. In analogy with the original SIR model, the *reporting rate* would be the proportion of interested individuals who end up actually searching about the event online.

We include two latent susceptible states $S_1$ and $S_2$. Intuitively, these two states are meant to differentiate between individuals who obtain their news from social media and individuals who may follow more traditional news sources. Since information spreads faster through social media than through sources such as news networks and newspapers [[7](https://larswillnat.files.wordpress.com/2014/05/2013-american-journalist-key-findings.pdf), [8](https://www.journalism.org/2015/07/14/the-evolving-role-of-news-on-twitter-and-facebook/)], we have included a potential delay to represent individuals who are "late" in hearing about the short squeeze. Individuals progress from $S_1$ to either $S_2$ or the infected state $I$, and from $S_2$ to $I$ only.

Finally, consider that the popularity of a trend can *resurge* long after the initial event; indeed, this behavior seems to be evident in the data. Because of this, we suppose that recovered individuals (represented by the state $R$) move back into the initial susceptible state $S_1$ after some time. Our model is summarized in the diagram below.

<div style='margin:40px'>
<center>
![Figure 1: Model Diagram](model.png)
</center>
</div>


## 3.2. Mathematical Formulation

The number of individuals in each compartment, at each time point $t$ are given by the following equations:

\begin{align*}
S_1(t) &= S_1(0) + N_{RS_1}(t) - N_{S_1S_2}(t) - N_{S_1I}(t) \\
S_2(t) &= S_2(0) + N_{S_1S_2}(t) - N_{S_2I}(t) \\
I(t) &= I(0) + N_{S_1I}(t) + N_{S_2I}(t) - N_{IR}(t) \\
R(t) &= R(0) + N_{IR}(t) - N_{RS_1}(t) \\
\end{align*}

where $N_{XY}$ is a counting process that represents the number of individuals transitioning from state $X$ to state $Y$. The associated compartment transfer rates are given by:

\begin{align*}
\frac{dN_{S_1S_2}(t)}{dt} &= \mu_{S_1S_2}S_1(t) \\
\frac{dN_{S_1I}(t)}{dt} &= \mu_{S_1I}(t)S_1(t) = \beta_1I(t)S_1(t) \\
\frac{dN_{S_2I}(t)}{dt} &= \mu_{S_2I}(t)S_2(t) = \beta_2I(t)S_2(t)\\
\frac{dN_{IR}(t)}{dt} &= \mu_{IR}I(t) \\
\frac{dN_{RS_1}(t)}{dt} &= \mu_{RS_1}R(t) \\
\end{align*}

We use Euler's method to compute approximations to the above rates. In particular, each transfer rate is discretely approximated by a binomial distribution with exponential transition probabilities:

\begin{align*}
\Delta N_{S_1S_2}(t) &\sim \mathrm{Bin}(S_1, 1 - e^{-\mu_{S_1S_2}\Delta t}) \\
\Delta N_{S_1I}(t) &\sim \mathrm{Bin}(S_1, 1 - e^{-\beta_1\frac{I}{N}\Delta t}) \\
\Delta N_{S_2I}(t) &\sim \mathrm{Bin}(S_2, 1 - e^{-\beta_2\frac{I}{N}\Delta t}) \\
\Delta N_{IR}(t) &\sim \mathrm{Bin}(I, 1 - e^{-\mu_{IR}\Delta t}) \\
\Delta N_{RS_1}(t) &\sim \mathrm{Bin}(R, 1 - e^{-\mu_{RS_1}\Delta t}) \\
\end{align*}

------------

## 3.3. Simulations

```{r model implement, echo=FALSE}
sir_step <- Csnippet("double dN_S1S2 = rbinom(S1, 1-exp(-mu_S1S2*dt));
                      double dN_S1I = rbinom(S1 - dN_S1S2, 1-exp(-Beta1*I/N*dt));
                      double dN_S2I = rbinom(S2, 1-exp(-Beta2*I/N*dt));
                      double dN_IR = rbinom(I, 1-exp(-mu_IR*dt));
                      double dN_RS1 = rbinom(R, 1-exp(-mu_RS1*dt));
                      S1 += dN_RS1 - (dN_S1I + dN_S1S2);
                      S2 += dN_S1S2 - dN_S2I;
                      I += dN_S1I + dN_S2I - dN_IR;
                      R += dN_IR - dN_RS1;
                      H += dN_IR;")
  
sir_init <- Csnippet("S1 = nearbyint(eta*N);
                      S2 = 0;
                      I = 1;
                      R = 0; 
                      H = 0;")

dgme <- Csnippet("lik = dbinom(count,H,rho,give_log);
                 ")

rgme <- Csnippet("count = rbinom(H,rho);")

gme <- as.data.frame(cbind(day = 1:nrow(gme), count=gme$count))

gme %>% 
  pomp(times="day",t0=0,
       rprocess=euler(sir_step,delta.t=1/6), # update once every 4 hours
       rinit=sir_init,
       rmeasure=rgme,
       dmeasure=dgme,
       accumvars="H", 
       partrans=parameter_trans(log=c("Beta1", "Beta2", "mu_S1S2", "mu_IR", "mu_RS1"),logit=c("rho","eta")),
       statenames=c("S1","S2","I","R","H"),
       paramnames=c("Beta1", "Beta2", "mu_S1S2", "mu_IR", "mu_RS1", "eta", "rho","N"),
       cdir=".", cfile="SSIR") -> SSIR
```

We simulate 50 trajectories from some manually chosen parameters in order to check the validity of our model. Noticeably, many of the trajectories seem to peak early, then level off around some smaller, nonzero value, much like the real data. Thus, we conclude that our model is reasonable, and continue on to conduct a parameter search.

```{r simulate, echo=FALSE}
params <- c(Beta1=1,Beta2=1,mu_S1S2=0.5,mu_IR=0.5,mu_RS1=0.02,rho=0.1,eta=0.8,N=20000)

SSIR %>%
  simulate(params=params, nsim=50,format="data.frame",include.data=TRUE) -> sims

sims %>% ggplot(aes(x=day,y=count,group=.id,color=.id=="data"))+ geom_line()+
guides(color=FALSE)
```

-----------

# 4. Model Selection

### Variable Parameters.

The model parameters we have to estimate are:

- $\beta_1$ — the information transmission rate through social media
- $\beta_2$ — the information transmission rate through "traditional" news sources
- $\mu_{S_1S_2}$ — representing a latent delay in the susceptible population
- $\mu_{IR}$ — the recovery rate ("hype factor")
- $\mu_{RS_1}$ — the resurgence rate
- $\eta$ — the initial susceptible fraction 

### Fixed Parameters.

Since the search frequency data are normalized to have maximum $100$, it is difficult to interpret the reporting rate and population size. However, Statista tells us that there were 267 million unique Google users from the US in 2020  [[9](https://www.statista.com/topics/1001/google/)]. Under some reasonable assumptions <sup style='color:gray'> [\*] </sup>, this leads to an estimated reporting rate of $\rho = 0.8$, which we will fix for simplicity. We will still let the population size $N$ vary so as to not overconstrain the model, but we can essentially ignore the resulting estimate.

<sup style='color:gray'>The assumptions are: 1) every unique user searched about the GME short squeeze and 2) all 328 million US residents have heard of the event at some point (aka, was once "infected").</sup>

### Global Search.

We define an optimal set of parameters as one that maximizes the likelihood of the model. To do this, we use iterated filtering -- the IF2 algorithm -- to simulate and evaluate the likelihood of many randomly-chosen candidate parameter sets. As a benchmark, we compute the likelihood of the parameter set used in the simulation in the previous section:

```{r iterations, echo=FALSE}
# helps us debug -- we want to use 3 when actually running

run_level <- 1
Np <- switch(run_level, 100, 1e3, 5e3)
Nmif <- switch(run_level, 10, 100, 200)
Nreps_eval <- switch(run_level, 2, 10, 20)
Nreps_global <- switch(run_level, 10, 20, 100)
Nsim <- switch(run_level, 50, 100, 500)

```

```{r}
SSIR %>% pfilter(Np=2000, params=params) -> pf

#fixed_params <- c(N=2000, rho=0.8)
paste0('log-likelihood: ', logLik(pf))
```

```{r, echo=FALSE, cache=TRUE}
# library(doParallel)
# registerDoParallel()
# library(doRNG)
# registerDoRNG(3899882)
# 
# 
# stew(file="pf1.rda",{
#   t1 <- system.time(
#     pf1 <- foreach(i=1:20,.packages='pomp') %dopar% 
#       pfilter(SSIR,Np=1000,params=params)
# ) })
# L1 <- logmeanexp(sapply(pf1,logLik),se=TRUE)
# L1
```

```{r, cache=TRUE}
# set.seed(2062379496)
# runif_design( lower=c(Beta=10,mu_EI = 0.3, mu_IR=0.1, rho=0.2,eta=0), 
#               upper=c(Beta=60,mu_EI = 0.7, mu_IR=0.7, rho=0.9,eta=0.4),
#               nseq=300) -> guesses
# mf1 <- mifs_local[[1]]
# 
# registerDoRNG(1270401374)
# foreach(guess=iter(guesses,"row"), .combine=rbind) %dopar% {
#   mf1 %>% mif2(params=c(unlist(guess),fixed_params)) %>% mif2(Nmif=100) -> mf 
#   replicate(10, mf %>% pfilter(Np=20) %>% logLik()) %>% logmeanexp(se=TRUE) -> ll
#   mf %>% coef() %>% bind_rows() %>% bind_cols(loglik=ll[1],loglik.se=ll[2]) 
#   } -> results
```

```{r grid, echo=FALSE}
# runif_design(
#   lower=c(Beta1=5, Beta2, rho=1,eta=0),
#   upper=c(Beta=80,rho=1,eta=0.4),
#   nseq=300
# ) -> guesses
# mf1 <- mifs_local[[1]]
```


-----------

# 5. Model Diagnostics

- do profile likelihood, etc.

-----------

# 6. The Effect of Stock Price

In the [Introduction]() and [Exploratory Data Analysis]() sections, we raised the question of how the value of $GME may affect or at least be associated with its search frequency. In this section, we'll include the stock price as a covariate and determine if the resulting model is a better fit.

## 6.1 Exploratory Data Analysis

```{r dataplot_price, echo=FALSE}
price$Date <- as.Date(price$Date, "%m/%d/%y")
price %>% ggplot(aes(x=Date,y=High)) + geom_line() + geom_point() +
ggtitle('$GME Stock Price, 1/6/2021 – 4/3/2021') +theme(plot.title = element_text(hjust = 0.5))
```

Plotting the stock price data, we see that the first half of the data strongly resembles the trend seen in the search frequency, but starts to diverge more in the latter half. Noticeably, the seasonality apparent in the search data is not nearly as present here.

We can plot the sample cross-correlation (CCF) between the stock price and the search frequency to more formally explore the relationship between the two.

```{r ccf, echo=FALSE}
# ccf between search and stock price
ccf(price$High, gme$count, main="CCF between Search Frequency and Stock Price")
```

We see that the search frequency of the term "gme" is most significantly correlated with the value of $GME about a week prior. There is also evidence that the search frequency *precedes* the stock price, indicating that there may be some sort of nonlinear, cyclical relationship between the two. However, we will stick with the hypothesis that the stock price is the one influencing the search frequency, and not vice versa.

## 6.2. Model Design

- include price as a covariate in this model

## 6.3. Model Selection 

note: the following code does not contain an accumulator variable $H$.

```{r}
data <- data_frame(day = 1:nrow(gme), count = gme$count)

gme_statenames <- c("S1", "S2", "I", "R")
gme_obsnames <- "count"
gme_t0 <- data$day[1]
```

```{r}
gme_covar <- covariate_table(
  t=data$day,
  P=price$High,
  times="t"
)

gme_rp_names <- c("Beta_1","Beta_2","mu_IR","mu_RS1","base","rho")
gme_ivp_names <- c("init1","init2")
gme_fp_names <- c("N")
gme_paramnames <- c(gme_rp_names,gme_ivp_names, gme_fp_names)
```

```{r}
# gme_rinit <- Csnippet("
#   S1 = round(init1*N);
#   S2 = round(init2*N);
#   I = 1;
#   R = fmax(N - (S1 + S2 + I), 0);
# ")
# 
# gme_rprocess <- Csnippet("
#   double dN_S1I = rbinom(S1, 1-exp(-Beta_1*dt));
#   double dN_S2I = rbinom(S2, 1-exp(-Beta_2*dt));
#   double dN_IR = rbinom(I,1-exp(-mu_IR*dt));
#   double dN_RS1 = rbinom(R, 1-exp(-mu_RS1*dt)
#   R += dN_IR;
#   I += (dN_S1I + dN_S2I) - dN_IR;
#   S2 = S1 - dN_S1I;
#   S1 = round(fmax(P, 0));
# ")
# 
# gme_dmeas <- Csnippet("lik = dbinom(2*count,4,1-exp(-base-rho*I),give_log);")
# gme_rmeas <- Csnippet("count = rbinom(4,1-exp(-base-rho*I))/2;")
# 
# gme_partrans <- parameter_trans(
#   log=c("Beta_1","Beta_2","mu_IR","mu_RS1","base", "rho"),
#   logit=c("init1","init2")
# )
# 
# SSIR <- pomp(
#   data,
#   times = "day",
#   t0 = gme_t0,
#   rinit = gme_rinit,
#   rprocess = euler(step.fun = gme_rprocess, delta.t=1),
#   rmeasure = gme_rmeas,
#   dmeasure = gme_dmeas,
#   covar = gme_covar,
#   obsnames = gme_obsnames,
#   statenames = gme_statenames,
#   paramnames = gme_paramnames,
#   partrans = gme_partrans
# )
```




- global parameter search

<!-- Model Diagnostics ??? -->

## 6.4. Model Comparison

- compare likelihood of this one to likelihood of above model (use Wilks' approximation)

------------

# 7. Conclusion

* compare $\beta_1$ to $\beta_2$ - make conclusion about speed of info transmission between news sources
* interpret $\mu_{IR}$, rate of recovery (hype factor) - how fast do trends fade?
* interpret $\eta$, initial susceptible fraction - what is the reach of one influential person?
* interpret $\mu_{RS_1}$, the resurgence rate - how often / likely to trends reoccur?

* is there evidence that incorporating stock price results in a better fit? aka, is there evidence that stock price is an external forcing variable for search frequency?

* how well was the data modeled by the SIRS model? What can we say about the relationship between information and disease?

------------

# 8. Limitations 

- make resurgence rate constant $\mu_{RS_1}$ time-varying
- make reporting rate $\rho$ time-varying
- reverse causation - although somewhat beyond the scope of this project, the greater question is: can social trends have an impact of stock prices, or do the stock prices determine such trends in the first place (...or both)? 

------------

# 9. References  

[1] [Wang, Lin & Wood, Brendan](https://www.sciencedirect.com/science/article/pii/S0307904X11002824), "An Epidemiological Approach to Model the Viral Propagation of Memes". <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *Applied Mathematical Modelling*, Volume 35, Issue 11: 5442-5447. November 11, 2011.

[2] [Stanford University](https://engineering.stanford.edu/magazine/article/how-fake-news-spreads-real-virus), "How Fake News Spreads Like a Real Virus". October 9, 2019.

[3] [Wikipedia](https://en.wikipedia.org/wiki/GameStop_short_squeeze), "GameStop Short Squeeze". Accessed April 8, 2021.

[4] [Google Trends](https://trends.google.com/trends/), Google. Accessed April 6, 2021.

[5] [Yahoo Finance](https://finance.yahoo.com/), Yahoo. Accessed April 9, 2021.

[6] [Investopedia](https://www.investopedia.com/terms/w/weekendeffect.asp), "Weekend Effect". Accessed April 13, 2021.

[7] [Indiana University](https://larswillnat.files.wordpress.com/2014/05/2013-american-journalist-key-findings.pdf), “The American Journalist in the Digital Age: Key Findings”. 2014.

[8] [Pew Research Center](https://www.journalism.org/2015/07/14/the-evolving-role-of-news-on-twitter-and-facebook/), “The Evolving Role of News on Twitter and Facebook”. July 14, 2015.

[9] [Statista](https://www.statista.com/topics/1001/google/), "Google -- Statistics & Facts". February 25, 2021.

<div style='margin-top: 25px'>
Our project took inspiration from the following former STATS 531 projects: [W20 Project 32](https://ionides.github.io/531w20/final_project/Project32/final.html) for the SIR model with delay, and <br> [W18 Project 13](https://ionides.github.io/531w18/final_project/13/final.html) and [W20 Project 34](https://ionides.github.io/531w20/final_project/Project34/final.html) for the idea of applying epidemiological models to information spread. All uncited statistical <br> methods, as well as the code used in this project, were adapted from the [STATS 531](https://ionides.github.io/531w21/) class notes.
</div>
----

We collaborated on all sections of the project with approximately equal contribution. Eric took the lead on the Introduction, Model Design, Conclusion, and Limitations, while Bella took the lead on Model Selection and Model Diagnostics. We usually worked independently on the above sections, but then met up both during class and outside of class to discuss our reasoning and share ideas / feedback. We would also hold pair programming sessions over Zoom in order to debug code. 