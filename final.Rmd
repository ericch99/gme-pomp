---
title: "Information Epidemics: Modeling Search Trends during the GameStop Short Squeeze Using Stochastic Compartmental Models"
subtitle: STATS 531 Final Project (W21)
author: "Eric Chen, Bella Chen"
output:
  html_document:
    toc: yes
    toc_depth: 3
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	warning = FALSE,
	message = FALSE,
	include = TRUE,
	comment='',
	fig.align = "center"
)
```

```{r load_packages, include=FALSE}
library(pomp)
library(tidyverse)

read_csv('gme.csv') -> gme
read_csv('price.csv') -> price
```

## Introduction

Social scientists have long drawn comparisons between the spread of information and the spread of disease. Methods from epidemiology have been used to model such dynamics by treating an idea, meme, or news event as the infectious agent; this analogy has only grown in popularity with the explosive growth of social media  [[1](https://www.sciencedirect.com/science/article/pii/S0307904X11002824), [2](https://engineering.stanford.edu/magazine/article/how-fake-news-spreads-real-virus)].

In this project, we apply epidemiological methods -- specifically, compartmental models -- to model information flow during the 2021 GameStop short squeeze, an event in which thousands of Reddit users coordinated to buy shares of the gaming company GameStop, having anticipated a short by major hedge funds. The resulting jump in stock price caused the hedge funds to incur significant losses, resulting in a flurry of attention on both social media sites and major news networks [[3](https://en.wikipedia.org/wiki/GameStop_short_squeeze)].

We use keyword search frequency as a proxy for the "spread" of news. The data was downloaded from Google Trends, consisting of daily measurements over a 90-day period [[4](https://trends.google.com/trends/)]. We used the following search terms associated with the event: "gme", "gamestop", "dogecoin", "squeeze", "stocks", "wallstreetbets", and "wsb". 

<div style='color:red'>
[something about our goals here]
</div>
------------

## Exploratory Data Analysis

```{r, echo=FALSE}
# set count = 0 for those <1
gme$count[as.numeric(gme$count) %>% is.na()] <- 0
gme$count <- as.numeric(gme$count)
gme %>% ggplot(aes(x=date,y=count)) + geom_line() + geom_point()
```
  The searching count of "GME" reaches its peak in late January, followed by multiple small bumps in the following months. Especially towards the latter half, the trend shows strong weekly seasonality. 
  ? may be due to most stock exchanges operates during weekdays, not related to GME short squeeze 
  
<div style='color:red'>

If we want to include ARMA, the trend doesn't look stationary may need transformation

</div>

```{r, echo=FALSE}
# acf of gme search 
acf(gme$count, lag.max = 40, main="auto-correlation of GME search")
```
The ACF plot indicates that searching trends is mostly associated with its count within five days.  
```{r, echo=FALSE}
# ccf between search and stock price
ccf(price$High, gme$count, main="cross-correlation between GME search and GME stock price")
```

  The cross correlation plot shows that popularity of the searching term "GME" is most likely correlated with its stock price about a week ago. This latent effect makes us wonder if the spread of information can be modeled by an SIR model with delay.  
  

```{r echo=FALSE}
# gme %>%
#   ggplot(aes(date, count)) + 
#   geom_line(color='#69b3a2', size=1) +
#     ylab("normalized search frequency") +
#     ggtitle('Normalized Search Frequency of the Term "gme"') +
#     theme(plot.title=element_text(hjust=0.5))
```

<div style='color:red'>

TODO: fix data types in dataframe for plotting

[comments about time series here]

[could also include ACF / seasonality analysis]

</div>

------------

## Model Design

### Model Structure

We model our data with a compartmental model, commonly used in epidemiological studies -- specifically, we use a **SIRS model with delay**.

We define *susceptible* in this context to mean individuals who have not yet heard about the short squeeze, *infected* to mean individuals who have heard about the short squeeze, and *recovered* to refer to individuals who have lost interest in the short squeeze and have no further inclination to search about it. In analogy with the original SIR model, the *reporting rate* would be the proportion of individuals who end up actually searching about the event online.

We include two latent susceptible states $S_1$ and $S_2$. Intuitively, these two states are meant to differentiate between individuals who obtain their news from social media and individuals who may follow more traditional news sources. Prior research has shown that information spreads faster through social media than through sources such as news networks and newspapers <span style='color:red'>[CITATION NEEDED]</span>. Thus, we have included a potential delay to represent individuals who are "late" in hearing about the short squeeze. Individuals progress from $S_1$ to either $S_2$ or the infected state $I$, and from $S_2$ to $I$ only.

Finally, consider that the popularity of a trend can resurge long after the initial event; indeed, this behavior seems to be evident in the data <span style='color:red'>[CITATION NEEDED]</span>. Because of this, we suppose that recovered individuals (represented by the state $R$) move back into the initial susceptible state $S_1$ after some time. Our model is summarized in the diagram below.

<div style='margin:40px'>
<center>
![Figure 1: Model Diagram](model.png)
</center>
</div>


### Mathematical Formulation

The number of individuals in each compartment, at each time point $t$ are given by the following equations:

\begin{align*}
S_1(t) &= S_1(0) + N_{RS_1}(t) - N_{S_1S_2}(t) - N_{S_1I}(t) \\
S_2(t) &= S_2(0) + N_{S_1S_2}(t) - N_{S_2I}(t) \\
I(t) &= I(0) + N_{S_1I}(t) + N_{S_2I}(t) - N_{IR}(t) \\
R(t) &= R(0) + N_{IR}(t) - N_{RS_1}(t) \\
\end{align*}

where $N_{XY}$ is a counting process that represents the number of individuals transitioning from state $X$ to state $Y$. The associated compartment transfer rates are given by:

\begin{align*}
\frac{dN_{S_1S_2}(t)}{dt} &= \mu_{S_1S_2}S_1(t) \\
\frac{dN_{S_1I}(t)}{dt} &= \mu_{S_1I}(t)S_1(t) = \beta_1I(t)S_1(t) \\
\frac{dN_{S_2I}(t)}{dt} &= \mu_{S_2I}(t)S_2(t) = \beta_2I(t)S_2(t)\\
\frac{dN_{IR}(t)}{dt} &= \mu_{IR}I(t) \\
\frac{dN_{RS_1}(t)}{dt} &= \mu_{RS_1}R(t) \\
\end{align*}

We use Euler's method to compute approximations to the above rates. In particular, each transfer rate is discretely approximated by a binomial distribution with exponential transition probabilities:

\begin{align*}
\Delta N_{S_1S_2}(t) &\sim \mathrm{Binomial}(S_1, 1 - e^{-\mu_{S_1S_2}\Delta t}) \\
\Delta N_{S_1I}(t) &\sim \mathrm{Binomial}(S_1, 1 - e^{-\beta_1\frac{I}{N}\Delta t}) \\
\Delta N_{S_2I}(t) &\sim \mathrm{Binomial}(S_2, 1 - e^{-\beta_2\frac{I}{N}\Delta t}) \\
\Delta N_{IR}(t) &\sim \mathrm{Binomial}(I, 1 - e^{-\mu_{IR}\Delta t}) \\
\Delta N_{RS_1}(t) &\sim \mathrm{Binomial}(R, 1 - e^{-\mu_{RS_1}\Delta t}) \\
\end{align*}

<div style='color:red'>
COMMENTS:
* should $\mu_{RS_1}$ depend on $t$?
</div>

------------

## References  

[1] [Wang, Lin & Wood, Brendan](https://www.sciencedirect.com/science/article/pii/S0307904X11002824), "An Epidemiological Approach to Model the Viral Propagation of Memes". November 11, 2011.

[2] [Stanford Engineering](https://engineering.stanford.edu/magazine/article/how-fake-news-spreads-real-virus), "How Fake News Spreads Like a Real Virus". October 9, 2019.

[3] [Wikipedia](https://en.wikipedia.org/wiki/GameStop_short_squeeze), "GameStop Short Squeeze". Accessed April 8, 2021.

[4] [Google Trends](https://trends.google.com/trends/), Google. Accessed April 6, 2021.

Our project took inspiration from the following former STATS 531 projects: [W20 Project 32](https://ionides.github.io/531w20/final_project/Project32/final.html) for the SIR model with delay, and <br> [W18 Project 13](https://ionides.github.io/531w18/final_project/13/final.html) and [W20 Project 34](https://ionides.github.io/531w20/final_project/Project34/final.html) for the idea of applying epidemiological models to information spread.

*All uncited statistical methods were obtained by consulting the [STATS 531](https://ionides.github.io/531w21/) class notes.*

