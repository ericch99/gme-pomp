---
title: "Information Epidemics: Modeling Search Trends during the GameStop Short Squeeze Using Stochastic Compartmental Models"
subtitle: STATS 531 Final Project (W21)
author: "Eric Chen, Bella Chen"
output:
  html_document:
    toc: yes
    toc_depth: 3
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	warning = FALSE,
	message = FALSE,
	include = TRUE,
	comment='',
	fig.align = "center"
)
```

```{r load_packages, include=FALSE}
library(pomp)
library(tidyverse)

read_csv('gme.csv') -> gme
read_csv('price.csv') -> price
```

## Introduction

Social scientists have long drawn comparisons between the spread of information and the spread of disease. Methods from epidemiology have been used to model such dynamics by treating an idea, meme, or news event as the infectious agent; this analogy has only grown in popularity with the explosive growth of social media  [[1](https://www.sciencedirect.com/science/article/pii/S0307904X11002824), [2](https://engineering.stanford.edu/magazine/article/how-fake-news-spreads-real-virus)].

In this project, we apply epidemiological methods -- specifically, compartmental models -- to model information flow during the 2021 GameStop short squeeze, an event in which thousands of Reddit users coordinated to buy shares of the gaming company GameStop, having anticipated a short by major hedge funds. The resulting jump in stock price caused the hedge funds to incur significant losses, resulting in a flurry of attention on both social media sites and major news networks [[3](https://en.wikipedia.org/wiki/GameStop_short_squeeze)].

We use search frequency of the term "gme" as a proxy for the spread of news. The data was downloaded from Google Trends [[4](https://trends.google.com/trends/)], consisting of normalized daily measurements over an 88-day period. We also include the price of GameStop stock (\$GME) for the same time period, downloaded from Yahoo Finance [[5](https://finance.yahoo.com/)].

The main objectives of this project are as follows: 

1. Model the information spread of a current event using an interpretable model.

2. Draw parallels between the spread of information and the spread of disease.

------------

## Exploratory Data Analysis

```{r dataplot, echo=FALSE}
# set count = 0 for those <1
gme$count[as.numeric(gme$count) %>% is.na()] <- 0
gme$count <- as.numeric(gme$count)
gme %>% ggplot(aes(x=date,y=count)) + geom_line() + geom_point()
```
  
First, we note that the data are normalized to have a maximum value of $100$. Understandably, the search frequency of the term "gme" peaks in late January, during the initial short squeeze. 

However, this is followed by multiple, smaller peaks in the following months. The trend in the latter half of the data seems to exhibit strong weekly seasonality, as shown in the plot of the smoothed periodogram below. (However, this may be because many stock exchanges are closed on weekends, and not something directly attributable to the GME hype itself.)

```{r spectrum, echo=FALSE}
smoothed <- spectrum(gme$count[45:88], spans=c(5,5,5), main="Periodogram (Smoothed)")
abline(v=1/7, lty=2, col='red')
paste0('period corresponding to maximum power (days): ', 1 / smoothed$freq[which.max(smoothed$spec)])
```

Plotting the sample autocorrelation (ACF) of the search frequency and the sample cross-correlation (CCF) between the stock price and the search frequency, we see that there is substantial correlation between values that are within one week of each other.

```{r acf, echo=FALSE}
# acf of gme search 
acf(gme$count, lag.max = 40, main="ACF of search frequency")
```

```{r ccf, echo=FALSE}
# ccf between search and stock price
ccf(price$High, gme$count, main="CCF between search frequency and stock price")
```

In particular, the CCF plot shows that the search frequency of the term "gme" is most significantly correlated with its stock price about a week ago. This seems to be in line with the seasonality analysis above. This latent effect suggests that the spread of information could be best modeled by including some sort of delay.

------------

## Model Design

### Model Structure

We model our data with a compartmental model, commonly used in epidemiological studies -- specifically, we use a **SIRS model with delay**.

We define *susceptible* in this context to mean individuals who have not yet heard about the short squeeze, *infected* to mean individuals who have heard about the short squeeze, and *recovered* to refer to individuals who have lost interest in the short squeeze and have no further inclination to search about it. In analogy with the original SIR model, the *reporting rate* would be the proportion of individuals who end up actually searching about the event online.

We include two latent susceptible states $S_1$ and $S_2$. Intuitively, these two states are meant to differentiate between individuals who obtain their news from social media and individuals who may follow more traditional news sources. Since information tends to spreads faster through social media than through sources such as news networks and newspapers, we have included a potential delay to represent individuals who are "late" in hearing about the short squeeze. Individuals progress from $S_1$ to either $S_2$ or the infected state $I$, and from $S_2$ to $I$ only.

Finally, consider that the popularity of a trend can resurge long after the initial event; indeed, this behavior seems to be evident in the data. Because of this, we suppose that recovered individuals (represented by the state $R$) move back into the initial susceptible state $S_1$ after some time. Our model is summarized in the diagram below.

<div style='margin:40px'>
<center>
![Figure 1: Model Diagram](model.png)
</center>
</div>


### Mathematical Formulation

The number of individuals in each compartment, at each time point $t$ are given by the following equations:

\begin{align*}
S_1(t) &= S_1(0) + N_{RS_1}(t) - N_{S_1S_2}(t) - N_{S_1I}(t) \\
S_2(t) &= S_2(0) + N_{S_1S_2}(t) - N_{S_2I}(t) \\
I(t) &= I(0) + N_{S_1I}(t) + N_{S_2I}(t) - N_{IR}(t) \\
R(t) &= R(0) + N_{IR}(t) - N_{RS_1}(t) \\
\end{align*}

where $N_{XY}$ is a counting process that represents the number of individuals transitioning from state $X$ to state $Y$. The associated compartment transfer rates are given by:

\begin{align*}
\frac{dN_{S_1S_2}(t)}{dt} &= \mu_{S_1S_2}S_1(t) \\
\frac{dN_{S_1I}(t)}{dt} &= \mu_{S_1I}(t)S_1(t) = \beta_1I(t)S_1(t) \\
\frac{dN_{S_2I}(t)}{dt} &= \mu_{S_2I}(t)S_2(t) = \beta_2I(t)S_2(t)\\
\frac{dN_{IR}(t)}{dt} &= \mu_{IR}I(t) \\
\frac{dN_{RS_1}(t)}{dt} &= \mu_{RS_1}R(t) \\
\end{align*}

We use Euler's method to compute approximations to the above rates. In particular, each transfer rate is discretely approximated by a binomial distribution with exponential transition probabilities:

\begin{align*}
\Delta N_{S_1S_2}(t) &\sim \mathrm{Binomial}(S_1, 1 - e^{-\mu_{S_1S_2}\Delta t}) \\
\Delta N_{S_1I}(t) &\sim \mathrm{Binomial}(S_1, 1 - e^{-\beta_1\frac{I}{N}\Delta t}) \\
\Delta N_{S_2I}(t) &\sim \mathrm{Binomial}(S_2, 1 - e^{-\beta_2\frac{I}{N}\Delta t}) \\
\Delta N_{IR}(t) &\sim \mathrm{Binomial}(I, 1 - e^{-\mu_{IR}\Delta t}) \\
\Delta N_{RS_1}(t) &\sim \mathrm{Binomial}(R, 1 - e^{-\mu_{RS_1}\Delta t}) \\
\end{align*}

<div style='color:red'>
COMMENTS:
* should $\mu_{RS_1}$ depend on $t$?
</div>

------------

## Parameter Search



------------

## References  

[1] [Wang, Lin & Wood, Brendan](https://www.sciencedirect.com/science/article/pii/S0307904X11002824), "An Epidemiological Approach to Model the Viral Propagation of Memes". November 11, 2011.

[2] [Stanford Engineering](https://engineering.stanford.edu/magazine/article/how-fake-news-spreads-real-virus), "How Fake News Spreads Like a Real Virus". October 9, 2019.

[3] [Wikipedia](https://en.wikipedia.org/wiki/GameStop_short_squeeze), "GameStop Short Squeeze". Accessed April 8, 2021.

[4] [Google Trends](https://trends.google.com/trends/), Google. Accessed April 6, 2021.

[5] [Yahoo Finance](https://finance.yahoo.com/), Yahoo. Accessed April 9, 2021.

Our project took inspiration from the following former STATS 531 projects: [W20 Project 32](https://ionides.github.io/531w20/final_project/Project32/final.html) for the SIR model with delay, and <br> [W18 Project 13](https://ionides.github.io/531w18/final_project/13/final.html) and [W20 Project 34](https://ionides.github.io/531w20/final_project/Project34/final.html) for the idea of applying epidemiological models to information spread.

*All uncited statistical methods were obtained by consulting the [STATS 531](https://ionides.github.io/531w21/) class notes.*

